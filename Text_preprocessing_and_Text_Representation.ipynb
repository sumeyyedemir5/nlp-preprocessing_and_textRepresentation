{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jr0ca7vpiH6CoZo0OL9RQ96B5hfLSrtX",
      "authorship_tag": "ABX9TyNN2u9iirgNHYaEHnR2fC4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumeyyedemir5/nlp-preprocessing_and_textRepresentation/blob/main/Text_preprocessing_and_Text_Representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "id": "kN_Ic4yjhEIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# Stemming: Kelimeyi eklerinden ayırıp köküne iner (örn: \"running\" -> \"run\")\n",
        "stemmer = PorterStemmer()\n",
        "words= [\"running\",\"runner\",\"runs\",\"go\",\"went\"]\n",
        "stems = [stemmer.stem(w) for w in words]\n",
        "stems"
      ],
      "metadata": {
        "id": "xt-2FnnBuC3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Örnek kelimeler\n",
        "words = [\"running\", \"runner\", \"ran\", \"runs\", \"better\", \"go\", \"went\"]\n",
        "# Lemmatization: Kelimeyi sözlükteki kök haline (lemma) çevirir (Daha anlamlıdır, örn: \"went\" -> \"go\")\n",
        "lemmas = [lemmatizer.lemmatize(w, pos=\"v\") for w in words]\n",
        "# 'v' fiil olarak kök bulmasını sağlar\n",
        "\n",
        "print(\"Lemma result: \",lemmas)\n"
      ],
      "metadata": {
        "id": "ErsDHDMOhFYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "# Stopwords: \"this, is, an, the\" gibi tek başına anlam ifade etmeyen yaygın kelimelerin temizlenmesi\n",
        "stop_words_eng = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "jKXap9b9uJLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is an example of removing stop words from a text document\"\n",
        "filtered_text = [word for word in text.split() if word.lower() not in stop_words_eng]\n",
        "filtered_text"
      ],
      "metadata": {
        "id": "SzPpap3JuNlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt_tab\")\n",
        "text = \"Hello World, 2025\"\n",
        "# Tokenization: Metni kelimelere (word) veya cümlelere (sentence) parçalama işlemidir\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "sentence_tokens = nltk.sent_tokenize(text)\n",
        "word_tokens"
      ],
      "metadata": {
        "id": "H5Xvama5uRp9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metin Temsili (Text Representation)\n",
        "Metinleri sayılara dönüştürme yöntemleridir.\n",
        "1.   **BoW (Bag of Words)**\n",
        "\n",
        "* Kelimelerin cümle içindeki sırasını önemsemeden sadece frekansına (kaç kere geçtiğine) bakar."
      ],
      "metadata": {
        "id": "61ZKeCWNyLZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "documents = [\n",
        "    \"Kedi evde\",\n",
        "    \"Kedi bahçede\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "x= vectorizer.fit_transform(documents)\n",
        "print(\"kelime kümesi: \", vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "xRoKTa0ryGg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vektör kümesi:\\n\",x.toarray())"
      ],
      "metadata": {
        "id": "J0ZoQHL5zoJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.satır (\"Kedi evde\") → [0 1 1]**\n",
        "\n",
        "\"bahçede\" kelimesi: 0 kez\n",
        "\n",
        "\"evde\" kelimesi: 1 kez\n",
        "\n",
        "\"kedi\" kelimesi: 1 kez\n",
        "\n",
        "**2.satır (\"Kedi bahçede\") → [1 0 1]**\n",
        "\n",
        "\"bahçede\": 1 kez\n",
        "\n",
        "\"evde\": 0 kez\n",
        "\n",
        "\"kedi\": 1 kez"
      ],
      "metadata": {
        "id": "PXYWgSjeqRgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IMDB Dataset.csv\", encoding=\"utf-8\")\n",
        "df = df.head(50)\n",
        "df"
      ],
      "metadata": {
        "id": "hI4KTNSx1OJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words_eng = set(stopwords.words(\"english\"))\n",
        "documents = df['review']\n",
        "labels = df[\"sentiment\"] #positive or negative\n",
        "#text cleaning func\n",
        "def clean_text(text):\n",
        "  text = text.lower() #lowercase conversion\n",
        "  text = re.sub(r\"\\d+\",\"\",text) #cleaning the numbers\n",
        "  text = re.sub(r\"[^\\w\\s]\",\"\",text) #cleaning the special chars\n",
        "  #cleaning short words\n",
        "  text = \" \".join([word for word in text.split() if len(word) > 2])\n",
        "  #cleaning stopwords\n",
        "  text = \" \".join([word for word in text.split() if word.lower() not in stop_words_eng])\n",
        "  return text\n",
        "cleaned_doc = [clean_text(doc) for doc in documents]"
      ],
      "metadata": {
        "id": "ONxUwrZn1NYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_doc"
      ],
      "metadata": {
        "id": "fELXHgQQIlhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X= vectorizer.fit_transform(cleaned_doc)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "vektor2 = X.toarray()[:2]\n",
        "vektor2"
      ],
      "metadata": {
        "id": "6MiaGFpnI47q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bow = pd.DataFrame(X.toarray(), columns = feature_names) #vektor temsili\n",
        "#kelime frekansı\n",
        "word_counts = X.sum(axis=0).A1\n",
        "word_freq = dict(zip(feature_names,word_counts))\n",
        "most_common_words = Counter(word_freq).most_common(5) #en çok tekrar eden 5 kelime\n",
        "most_common_words"
      ],
      "metadata": {
        "id": "YpLH3wuuKGxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.   TF-IDF (Term Frequency - inverse document frequency)**\n",
        "TF : kelimenin ne kadar sık geçtiğini ölçer\n",
        "IDF : kelimenin tüm belgedeki yaygınlığını ölçer. çok fazla bulunan kelimeler çok bilgi sağlamaz.\n"
      ],
      "metadata": {
        "id": "rTGj1dy6RMbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "doc = [\n",
        "    \"kedi çok tatlı bir hayvandır\",\n",
        "    \"kedi ve köpekler çok tatlı hayvanlardır\"\n",
        "]\n",
        "tfidfvector = TfidfVectorizer()\n",
        "X = tfidfvector.fit_transform(doc)\n",
        "feature_names = tfidfvector.get_feature_names_out()\n",
        "df_tfidf = pd.DataFrame(X.toarray(),columns = feature_names)\n",
        "df_tfidf"
      ],
      "metadata": {
        "id": "pewQ4eVjUHcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kelimenin metin içindeki önemini gösteren TF-IDF değerlerinin ortalaması"
      ],
      "metadata": {
        "id": "D7_Wb6aiWn9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kedi_tfidf = df_tfidf[\"kedi\"]\n",
        "kedi_mean_tfidf = np.mean(kedi_tfidf)\n",
        "kedi_mean_tfidf"
      ],
      "metadata": {
        "id": "75ubYp_3VulV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"/content/drive/MyDrive/sms_spam.csv\")\n",
        "df2"
      ],
      "metadata": {
        "id": "NG9wV5CMXJ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df2.text)\n",
        "X"
      ],
      "metadata": {
        "id": "ESkthwJHYKP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vectorizer.get_feature_names_out\n",
        "tfidf_score = X.mean(axis=0).A1 #ortalama TF-IDF değerleri\n",
        "df_tfidf = pd.DataFrame({\"word\":feature_names,\"score\":tfidf_score})\n",
        "df_tfidf"
      ],
      "metadata": {
        "id": "59hK30yfYgM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-GRAM MODELLİNG**"
      ],
      "metadata": {
        "id": "xI489pxJ8iTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "documents = [\n",
        "    \"bu bir örnek metindir\",\n",
        "    \"bu örnek metin doğal dil işlemeyi gösterir\"\n",
        "]\n",
        "vectorizer_unigram = CountVectorizer(ngram_range=(1,1))\n",
        "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
        "vectorizer_trigram = CountVectorizer(ngram_range=(3,3))\n",
        "\n",
        "X_unigram = vectorizer_unigram.fit_transform(documents)\n",
        "unigram_features = vectorizer_unigram.get_feature_names_out()\n",
        "unigram_features"
      ],
      "metadata": {
        "id": "DqabNSH16UVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bigram = vectorizer_bigram.fit_transform(documents)\n",
        "bigram_features = vectorizer_bigram.get_feature_names_out()\n",
        "bigram_features"
      ],
      "metadata": {
        "id": "Z0rScKsn7q-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_trigram = vectorizer_trigram.fit_transform(documents)\n",
        "trigram_features = vectorizer_trigram.get_feature_names_out()\n",
        "trigram_features"
      ],
      "metadata": {
        "id": "qWo8f-_o8N4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}