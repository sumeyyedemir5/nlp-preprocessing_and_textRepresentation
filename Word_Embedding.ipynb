{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QkK5MGr6Ks-eX8RHj_Cva94ixwyYLBtI",
      "authorship_tag": "ABX9TyNGqQzQYRN132mcjQrO3Lel",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumeyyedemir5/nlp-preprocessing_and_textRepresentation/blob/main/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORD EMBEDDİNG\n",
        "\n",
        "kelimeleri sayısal vektörlere dönüştürür. bu temsiller kelimeler arasındaki anlamsal ilişkileri yakalamayı hedefler.\n",
        "\n",
        "* **word2Vec** : Google tarafınfan geliştirilen ,kelimeleri vektörlere dönüştüren ve bu vektörleri dildeki ilişkileri yakalayacak şekilde eğiten modeldir.\n",
        "* **GloVe** : kelime gömme temsillerini kelime ortaklıklarını yakalayacak şekilde hesaplayan bir modeldir.\n",
        "* **FastText** : Facebook tarafından geliştirilen ve kelime gömme temsillerini alt birimlerini de dikkate alarak hesaplayan bir modeldir"
      ],
      "metadata": {
        "id": "xF9dvz-mQjUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Shef_KLZQiUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.utils import simple_preprocess #büyük küçük harf çevrimi ve tokenizasyon\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentences = [\n",
        "    \"kedi çok tatlı bir hayvandır\",\n",
        "    \"köpekler de tatlıdır\"\n",
        "]\n",
        "\n",
        "tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
        "tokenized_sentences\n"
      ],
      "metadata": {
        "id": "8echwD89DhZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = Word2Vec(sentences=tokenized_sentences,vector_size=50,window=5,min_count=1,sg=0)\n",
        "#vector_size = her kelime için oluşturulacak vektörlerin boyutunu belirler.boyut arttıkça yorumlama gücü artar\n",
        "#window = kelimeye en yakın kaç kelime dikkate alınacak?\n",
        "#min_count = 1 kelimeden sadece 1 adet bulunuyorsa o kelimenin dikkate alınmayacağı anlamına gelir.\n",
        "#sg=0 BoW algoritmasını kullanacak sg=1 ngram algoritması\n",
        "\n",
        "fasttext = FastText(sentences=tokenized_sentences,vector_size=50,window=5,min_count=1,sg=0)\n",
        "\n",
        "def plot_word_embadding(model,title):\n",
        "  word_vectors = model.wv # modelin öğrendiği tüm kelimeleri ve bunlara karşılık gelen sayısal vektörleri tutan sözlüktür\n",
        "  words = list(word_vectors.index_to_key)[:1000]\n",
        "  vectors = [word_vectors[word] for word in words]\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=3)\n",
        "  reduced_vectors = pca.fit_transform(vectors)\n",
        "\n",
        "  #3d görselleştirme\n",
        "  fig = plt.figure(figsize = (12,8))\n",
        "  ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "  #vektorler\n",
        "  ax.scatter(reduced_vectors[:,0],reduced_vectors[:,1],reduced_vectors[:,2])\n",
        "  #kelimeleri etiketler\n",
        "  for i, word in enumerate(words):\n",
        "    ax.text(reduced_vectors[i,0],reduced_vectors[i,1],reduced_vectors[i,2],word, fontsize=12)\n",
        "\n",
        "  ax.set_title(title)\n",
        "  ax.set_xlabel(\"Component 1\")\n",
        "  ax.set_ylabel(\"Component 2\")\n",
        "  ax.set_zlabel(\"Component 3\")\n",
        "  plt.show()\n",
        "\n",
        "plot_word_embadding(word2vec,\"Word2Vec\")\n",
        "plot_word_embadding(fasttext,\"FastText\")"
      ],
      "metadata": {
        "id": "FvkiGtVeIA0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import re"
      ],
      "metadata": {
        "id": "nh5MyVZbVEQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"/content/drive/MyDrive/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "-dvPXsh-SrOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df[\"review\"]"
      ],
      "metadata": {
        "id": "LNRAAT9gVR72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metin temizleme\n",
        "def clean_text(text):\n",
        "  text = re.sub(r\"\\d+\",\"\",text)\n",
        "  text = re.sub(r\"[^\\w\\s]\",\"\",text)\n",
        "  text = \" \".join([word for word in text.split() if len(word) > 2])\n",
        "  return text"
      ],
      "metadata": {
        "id": "mSqkRS4TS9DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_doc = [clean_text(doc) for doc in documents]\n",
        "#tokenizasyon\n",
        "tokenized_documents = [simple_preprocess(doc) for doc in cleaned_doc]"
      ],
      "metadata": {
        "id": "X0r9A1-AVxL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences = tokenized_documents, vector_size = 50,window =5, min_count=1, sg=0)\n",
        "word_vectors = model.wv\n",
        "\n",
        "words = list(word_vectors.index_to_key)[:500]\n",
        "vectors = [word_vectors[word] for word in words]"
      ],
      "metadata": {
        "id": "7YzfZFKkdBfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters= 3) #3 küme\n",
        "kmeans.fit(vectors)\n",
        "clusters = kmeans.labels_ #her kelimenin hangi kümeye ait olduğunu gösterir. örn: [0,0,1,1,2..]\n",
        "\n",
        "pca = PCA(n_components = 2) #analizi kolaylaştırmak için 50 boyutu 2 boyuta indiriyoruz.\n",
        "reduced_vectors = pca.fit_transform(vectors)"
      ],
      "metadata": {
        "id": "R0HjEDvnfywb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2d görselleştirme\n",
        "plt.figure()\n",
        "plt.scatter(reduced_vectors[:,0],reduced_vectors[:,1],c = clusters,cmap =\"viridis\" )\n",
        "centers = pca.transform(kmeans.cluster_centers_)\n",
        "plt.scatter(centers[:,0], centers[:,1], c=\"red\", marker =\"x\", s=80, label = \"merkez\")\n",
        "plt.legend()\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "  plt.text(reduced_vectors[i,0], reduced_vectors[i,1], word, fontsize=8)\n",
        "plt.title(\"word2Vec\")"
      ],
      "metadata": {
        "id": "rZJ1J7Imgjfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}