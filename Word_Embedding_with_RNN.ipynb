{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WainCgXvteoknI5V26hjjJ66tFAxvaQ8",
      "authorship_tag": "ABX9TyO4mpcst1b1fGdCvwuoAqki",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumeyyedemir5/nlp-preprocessing_and_textRepresentation/blob/main/Word_Embedding_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "K1wg7SD4thcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Maksimum Entropi Modelleri**\n"
      ],
      "metadata": {
        "id": "Jn73UTQ-ei07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify import MaxentClassifier\n",
        "train_data = [\n",
        "    ({\"love\":True, \"amazing\":True}, \"positive\"),\n",
        "    ({\"hate\":True, \"terrible\":True}, \"negative\"),\n",
        "    ({\"happy\":True, \"joy\":True}, \"positive\"),\n",
        "    ({\"sad\":True, \"depressed\":True}, \"negative\")\n",
        "]\n",
        "\n",
        "classifier = MaxentClassifier.train(train_data, max_iter = 10)\n",
        "test_sentence = \"I like this amazing movie\"\n",
        "\n",
        "features = {word: (word in test_sentence.lower().split()) for word in [\"love\",\"hate\",\"terrible\",\"happy\",\"joy\",\"sad\",\"depressed\"]}\n",
        "\n",
        "label = classifier.classify(features)\n",
        "label"
      ],
      "metadata": {
        "id": "DnHUtJwMe2Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORD EMBEDDİNGS\n",
        "\n",
        "\n",
        "**WORD2VEC Temel Modelleri**\n",
        "*  CBOW\n",
        "\n",
        "bir kelimenin bağlamındaki diğer kelimeleri kullanarak o kelimeyi tahmin etmeyi hedefler.\n",
        "*   Skip-gram model\n",
        "\n",
        "CBOW'un tam tersidir. Bir kelimeyi kullanarak o kelimenin bağlamında yer alan kelimeleri tahmin etmeyi hedefler.\n",
        "\n"
      ],
      "metadata": {
        "id": "KzbSSof1MhVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recurrent Neural Networks (RNN)**\n",
        "\n",
        "*her zaman adımında önceki zaman adımındaki bilgiyi saklayarak ve sonraki adımlarla bu bilgiyi güncelleyerek çalışırlar.*\n",
        "\n"
      ],
      "metadata": {
        "id": "Cku6gw4LTH6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense , Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I absolutely loved the movie, it was fantastic!\",\n",
        "        \"The plot was boring and predictable.\",\n",
        "        \"Great performances by the lead actors.\",\n",
        "        \"I didn't enjoy the film at all.\",\n",
        "        \"The cinematography was stunning and beautiful.\",\n",
        "        \"Terrible script and poor direction.\",\n",
        "        \"An emotional rollercoaster that kept me engaged.\",\n",
        "        \"The movie was too long and dragged on.\",\n",
        "        \"A masterpiece that exceeded my expectations.\",\n",
        "        \"I regret watching this movie.\",\n",
        "        \"The soundtrack was amazing and memorable.\",\n",
        "        \"The acting was wooden and unconvincing.\",\n",
        "        \"I laughed throughout, such a fun movie!\",\n",
        "        \"I almost fell asleep, it was that dull.\",\n",
        "        \"Brilliant storytelling and plot twists.\",\n",
        "        \"Disappointing and underwhelming.\",\n",
        "        \"The visuals were breathtaking and immersive.\",\n",
        "        \"Horrible pacing and confusing plot.\",\n",
        "        \"I was moved by the heartfelt scenes.\",\n",
        "        \"It felt like a waste of time.\",\n",
        "        \"Excellent character development.\",\n",
        "        \"Poor editing and awkward transitions.\",\n",
        "        \"The movie had a perfect balance of humor and drama.\",\n",
        "        \"I couldn't relate to any of the characters.\",\n",
        "        \"An inspiring and uplifting story.\",\n",
        "        \"The dialogues were cringe-worthy.\",\n",
        "        \"Highly entertaining and captivating from start to finish.\",\n",
        "        \"It was cliché and uninspired.\",\n",
        "        \"I would definitely watch it again.\",\n",
        "        \"I wish I hadn't spent money on this film.\"\n",
        "    ],\n",
        "    \"label\": [\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "# tokenize\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"text\"])\n",
        "sequences = tokenizer.texts_to_sequences(df[\"text\"]) # Cümleler sayı dizilerine dönüşür: \"I love\" -> [5, 12]\n",
        "word_index = tokenizer.word_index # Kelime-Sayı eşleşmesini saklar\n",
        "\n",
        "# Padding: Tüm cümleleri aynı uzunluğa getirir. RNN sabit boyutlu girdi ister.\n",
        "maxlen = max(len(seq) for seq in sequences)\n",
        "x= pad_sequences(sequences,maxlen=maxlen)\n",
        "\n",
        "# label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(data[\"label\"])\n",
        "\n",
        "#train_test_split\n",
        "X_train ,X_test, y_train, y_test = train_test_split(x ,y ,test_size= 0.3, random_state=42)\n",
        "\n",
        "# Word embedding\n",
        "# Word2Vec Eğitimi: Kelimelerin birbirine göre konumlarını belirler.\n",
        "sentences = [text.split() for text in data[\"text\"]]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix=np.zeros((len(word_index) +1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if word in word2vec_model.wv:\n",
        "    embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "#build RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index)+1, output_dim = embedding_dim, weights= [embedding_matrix],input_length = maxlen, trainable = False))\n",
        "model.add(SimpleRNN(100,return_sequences=False))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(X_train,y_train,epochs= 10, batch_size = 2,validation_data=(X_test,y_test))\n",
        "\n",
        "print(\" \")\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test loss: \", loss)\n",
        "print(\"Test accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "ILFZa9o9CILS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "suvYnJP1taX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(sentence):\n",
        "  seq = tokenizer.texts_to_sequences([sentence])\n",
        "  padded_seq = pad_sequences(seq,maxlen = maxlen)\n",
        "\n",
        "  prediction = model.predict(padded_seq)\n",
        "  predicted_class = (prediction > 0.5).astype(int)\n",
        "  label = \"pozitif\" if predicted_class[0][0] == 1 else \"negatif\"\n",
        "  return label\n",
        "\n",
        "sentence = \"The dialogues were cringe-worthy.\"\n",
        "result = classify_sentence(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "6AkTCt6AXvnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#--------TEXT CLASSIFIER ---------\n",
        "import pandas as pd\n",
        "df_spam = pd.read_csv(\"/content/drive/MyDrive/sms_spam.csv\",encoding=\"latin-1\")\n",
        "df_spam.head(5)"
      ],
      "metadata": {
        "id": "4xrICKhXVV1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam.columns"
      ],
      "metadata": {
        "id": "aCk07S95XM_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam.columns =[\"label\",\"text\"]"
      ],
      "metadata": {
        "id": "9TqRKvSgXSCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_spam.isnull().sum()"
      ],
      "metadata": {
        "id": "aVf9Dpw-X0sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "text = list(df_spam['text'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "corpus = []\n",
        "for i in range(len(text)):\n",
        "  r = re.sub(\"[^a-zA-Z]\",\" \",text[i])\n",
        "  r = r.lower()\n",
        "  r = r.split()\n",
        "  r = [word for word in r if word not in set(stopwords.words(\"english\"))]\n",
        "  r = [lemmatizer.lemmatize(word) for word in r]\n",
        "  r = \" \".join(r)\n",
        "  corpus.append(r)\n",
        "\n",
        "df_spam[\"text2\"] = corpus\n"
      ],
      "metadata": {
        "id": "xtzZYuZuYANl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X = df_spam[\"text2\"]\n",
        "y = df_spam[\"label\"]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33,random_state=42)\n"
      ],
      "metadata": {
        "id": "5XxH01bFbHY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "x_train = cv.fit_transform(X_train)\n"
      ],
      "metadata": {
        "id": "A7rA4UJFbsgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classifier training\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "E1swL1TccNWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = cv.transform(X_test)\n",
        "y_pred = classifier.predict(X_test)\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ],
      "metadata": {
        "id": "SzwoDVt6ce98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PV1hWZnKePCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}