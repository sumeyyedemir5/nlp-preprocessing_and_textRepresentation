{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jr0ca7vpiH6CoZo0OL9RQ96B5hfLSrtX",
      "authorship_tag": "ABX9TyMekQ6hXzD2Lt3sE+22D326",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumeyyedemir5/nlp-preprocessing_and_textRepresentation/blob/main/nlp_preprocessing_and_textRepresentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import nltk\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "id": "kN_Ic4yjhEIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# Stemming: Kelimeyi eklerinden ayırıp köküne iner (örn: \"running\" -> \"run\")\n",
        "stemmer = PorterStemmer()\n",
        "words= [\"running\",\"runner\",\"runs\",\"go\",\"went\"]\n",
        "stems = [stemmer.stem(w) for w in words]\n",
        "stems"
      ],
      "metadata": {
        "id": "xt-2FnnBuC3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Örnek kelimeler\n",
        "words = [\"running\", \"runner\", \"ran\", \"runs\", \"better\", \"go\", \"went\"]\n",
        "# Lemmatization: Kelimeyi sözlükteki kök haline (lemma) çevirir (Daha anlamlıdır, örn: \"went\" -> \"go\")\n",
        "lemmas = [lemmatizer.lemmatize(w, pos=\"v\") for w in words]\n",
        "# 'v' fiil olarak kök bulmasını sağlar\n",
        "\n",
        "print(\"Lemma result: \",lemmas)\n"
      ],
      "metadata": {
        "id": "ErsDHDMOhFYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "# Stopwords: \"this, is, an, the\" gibi tek başına anlam ifade etmeyen yaygın kelimelerin temizlenmesi\n",
        "stop_words_eng = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "jKXap9b9uJLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is an example of removing stop words from a text document\"\n",
        "filtered_text = [word for word in text.split() if word.lower() not in stop_words_eng]\n",
        "filtered_text"
      ],
      "metadata": {
        "id": "SzPpap3JuNlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt_tab\")\n",
        "text = \"Hello World, 2025\"\n",
        "# Tokenization: Metni kelimelere (word) veya cümlelere (sentence) parçalama işlemidir\n",
        "word_tokens = nltk.word_tokenize(text)\n",
        "sentence_tokens = nltk.sent_tokenize(text)\n",
        "word_tokens"
      ],
      "metadata": {
        "id": "H5Xvama5uRp9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metin Temsili (Text Representation)\n",
        "Metinleri sayılara dönüştürme yöntemleridir.\n",
        "1.   **BoW (Bag of Words)**\n",
        "\n",
        "* Kelimelerin cümle içindeki sırasını önemsemeden sadece frekansına (kaç kere geçtiğine) bakar."
      ],
      "metadata": {
        "id": "61ZKeCWNyLZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "documents = [\n",
        "    \"Kedi evde\",\n",
        "    \"Kedi bahçede\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "x= vectorizer.fit_transform(documents)\n",
        "print(\"kelime kümesi: \", vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "xRoKTa0ryGg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vektör kümesi:\\n\",x.toarray())"
      ],
      "metadata": {
        "id": "J0ZoQHL5zoJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.satır (\"Kedi evde\") → [0 1 1]**\n",
        "\n",
        "\"bahçede\" kelimesi: 0 kez\n",
        "\n",
        "\"evde\" kelimesi: 1 kez\n",
        "\n",
        "\"kedi\" kelimesi: 1 kez\n",
        "\n",
        "**2.satır (\"Kedi bahçede\") → [1 0 1]**\n",
        "\n",
        "\"bahçede\": 1 kez\n",
        "\n",
        "\"evde\": 0 kez\n",
        "\n",
        "\"kedi\": 1 kez"
      ],
      "metadata": {
        "id": "PXYWgSjeqRgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/IMDB Dataset.csv\", encoding=\"utf-8\")\n",
        "df = df.head(50)\n",
        "df"
      ],
      "metadata": {
        "id": "hI4KTNSx1OJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words_eng = set(stopwords.words(\"english\"))\n",
        "documents = df['review']\n",
        "labels = df[\"sentiment\"] #positive or negative\n",
        "#text cleaning func\n",
        "def clean_text(text):\n",
        "  text = text.lower() #lowercase conversion\n",
        "  text = re.sub(r\"\\d+\",\"\",text) #cleaning the numbers\n",
        "  text = re.sub(r\"[^\\w\\s]\",\"\",text) #cleaning the special chars\n",
        "  #cleaning short words\n",
        "  text = \" \".join([word for word in text.split() if len(word) > 2])\n",
        "  #cleaning stopwords\n",
        "  text = \" \".join([word for word in text.split() if word.lower() not in stop_words_eng])\n",
        "  return text\n",
        "cleaned_doc = [clean_text(doc) for doc in documents]"
      ],
      "metadata": {
        "id": "ONxUwrZn1NYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_doc"
      ],
      "metadata": {
        "id": "fELXHgQQIlhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X= vectorizer.fit_transform(cleaned_doc)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "vektor2 = X.toarray()[:2]\n",
        "vektor2"
      ],
      "metadata": {
        "id": "6MiaGFpnI47q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_bow = pd.DataFrame(X.toarray(), columns = feature_names) #vektor temsili\n",
        "#kelime frekansı\n",
        "word_counts = X.sum(axis=0).A1\n",
        "word_freq = dict(zip(feature_names,word_counts))\n",
        "most_common_words = Counter(word_freq).most_common(5) #en çok tekrar eden 5 kelime\n",
        "most_common_words"
      ],
      "metadata": {
        "id": "YpLH3wuuKGxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.   TF-IDF (Term Frequency - inverse document frequency)**\n",
        "TF : kelimenin ne kadar sık geçtiğini ölçer\n",
        "IDF : kelimenin tüm belgedeki yaygınlığını ölçer. çok fazla bulunan kelimeler çok bilgi sağlamaz.\n"
      ],
      "metadata": {
        "id": "rTGj1dy6RMbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "doc = [\n",
        "    \"kedi çok tatlı bir hayvandır\",\n",
        "    \"kedi ve köpekler çok tatlı hayvanlardır\"\n",
        "]\n",
        "tfidfvector = TfidfVectorizer()\n",
        "X = tfidfvector.fit_transform(doc)\n",
        "feature_names = tfidfvector.get_feature_names_out()\n",
        "df_tfidf = pd.DataFrame(X.toarray(),columns = feature_names)\n",
        "df_tfidf"
      ],
      "metadata": {
        "id": "pewQ4eVjUHcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kelimenin metin içindeki önemini gösteren TF-IDF değerlerinin ortalaması"
      ],
      "metadata": {
        "id": "D7_Wb6aiWn9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kedi_tfidf = df_tfidf[\"kedi\"]\n",
        "kedi_mean_tfidf = np.mean(kedi_tfidf)\n",
        "kedi_mean_tfidf"
      ],
      "metadata": {
        "id": "75ubYp_3VulV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"/content/drive/MyDrive/sms_spam.csv\")\n",
        "df2"
      ],
      "metadata": {
        "id": "NG9wV5CMXJ6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df2.text)\n",
        "X"
      ],
      "metadata": {
        "id": "ESkthwJHYKP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vectorizer.get_feature_names_out\n",
        "tfidf_score = X.mean(axis=0).A1 #ortalama TF-IDF değerleri\n",
        "df_tfidf = pd.DataFrame({\"word\":feature_names,\"score\":tfidf_score})\n",
        "df_tfidf"
      ],
      "metadata": {
        "id": "59hK30yfYgM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**N-GRAM MODELLİNG**"
      ],
      "metadata": {
        "id": "xI489pxJ8iTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "documents = [\n",
        "    \"bu bir örnek metindir\",\n",
        "    \"bu örnek metin doğal dil işlemeyi gösterir\"\n",
        "]\n",
        "vectorizer_unigram = CountVectorizer(ngram_range=(1,1))\n",
        "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
        "vectorizer_trigram = CountVectorizer(ngram_range=(3,3))\n",
        "\n",
        "X_unigram = vectorizer_unigram.fit_transform(documents)\n",
        "unigram_features = vectorizer_unigram.get_feature_names_out()\n",
        "unigram_features"
      ],
      "metadata": {
        "id": "DqabNSH16UVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_bigram = vectorizer_bigram.fit_transform(documents)\n",
        "bigram_features = vectorizer_bigram.get_feature_names_out()\n",
        "bigram_features"
      ],
      "metadata": {
        "id": "Z0rScKsn7q-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_trigram = vectorizer_trigram.fit_transform(documents)\n",
        "trigram_features = vectorizer_trigram.get_feature_names_out()\n",
        "trigram_features"
      ],
      "metadata": {
        "id": "qWo8f-_o8N4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORD EMBEDDİNG\n",
        "kelimeleri sayısal vektörlere dönüştürür. bu temsiller kelimeler arasındaki anlamsal ilişkileri yakalamayı hedefler.\n",
        "\n",
        "\n",
        "*   word2Vec : Google tarafınfan geliştirilen ,kelimeleri vektörlere dönüştüren ve bu vektörleri dildeki ilişkileri yakalayacak şekilde eğiten modeldir.\n",
        "*   GloVe : kelime gömme temsillerini kelime ortaklıklarını yakalayacak şekilde hesaplayan bir modeldir.\n",
        "*   FastText : Facebook tarafından geliştirilen ve kelime gömme temsillerini alt birimlerini de dikkate alarak hesaplayan bir modeldir\n",
        "\n"
      ],
      "metadata": {
        "id": "1zFKofhg830T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "WaOpznZVauty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim.utils import simple_preprocess #büyük küçük harf çevrimi ve tokenizasyon\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sentences = [\n",
        "    \"kedi çok tatlı bir hayvandır\",\n",
        "    \"köpekler de tatlıdır\"\n",
        "]\n",
        "\n",
        "tokenized_sentences = [simple_preprocess(sentence) for sentence in sentences]\n",
        "tokenized_sentences\n"
      ],
      "metadata": {
        "id": "8echwD89DhZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = Word2Vec(sentences=tokenized_sentences,vector_size=50,window=5,min_count=1,sg=0)\n",
        "#vector_size = her kelime için oluşturulacak vektörlerin boyutunu belirler.boyut arttıkça yorumlama gücü artar\n",
        "#window = kelimeye en yakın kaç kelime dikkate alınacak?\n",
        "#min_count = 1 kelimeden sadece 1 adet bulunuyorsao kelimenin dikkate alınmayacağı anlamına gelir.\n",
        "#sg=0 BoW algoritmasını kullanacak sg=1 engram algoritması\n",
        "\n",
        "fasttext = FastText(sentences=tokenized_sentences,vector_size=50,window=5,min_count=1,sg=0)\n",
        "\n",
        "def plot_word_embadding(model,title):\n",
        "  word_vectors = model.wv\n",
        "  words = list(word_vectors.index_to_key)[:1000]\n",
        "  vectors = [word_vectors[word] for word in words]\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=3)\n",
        "  reduced_vectors = pca.fit_transform(vectors)\n",
        "\n",
        "  #3d görselleştirme\n",
        "  fig = plt.figure(figsize = (12,8))\n",
        "  ax = fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "  #vektorler\n",
        "  ax.scatter(reduced_vectors[:,0],reduced_vectors[:,1],reduced_vectors[:,2])\n",
        "  #kelimeleri etiketler\n",
        "  for i, word in enumerate(words):\n",
        "    ax.text(reduced_vectors[i,0],reduced_vectors[i,1],reduced_vectors[i,2],word, fontsize=12)\n",
        "\n",
        "  ax.set_title(title)\n",
        "  ax.set_xlabel(\"Component 1\")\n",
        "  ax.set_ylabel(\"Component 2\")\n",
        "  ax.set_zlabel(\"Component 3\")\n",
        "  plt.show()\n",
        "\n",
        "plot_word_embadding(word2vec,\"Word2Vec\")\n",
        "plot_word_embadding(fasttext,\"FastText\")"
      ],
      "metadata": {
        "id": "FvkiGtVeIA0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import re"
      ],
      "metadata": {
        "id": "mmvgOBWaSJFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"/content/drive/MyDrive/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "-dvPXsh-SrOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = df[\"review\"]"
      ],
      "metadata": {
        "id": "E9FDYM6iS3PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metin temizleme\n",
        "def clean_text(text):\n",
        "  text = re.sub(r\"\\d+\",\"\",text)\n",
        "  text = re.sub(r\"[^\\w\\s]\",\"\",text)\n",
        "  text = \" \".join([word for word in text.split() if len(word) > 2])\n",
        "  return text"
      ],
      "metadata": {
        "id": "mSqkRS4TS9DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_documents = [clean_text(doc) for doc in documents]\n",
        "#tokenizasyon\n",
        "tokenized_documents = [simple_preprocess(doc) for doc in cleaned_documents]"
      ],
      "metadata": {
        "id": "f6iMBRxiTrEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences = tokenized_documents, vector_size = 50,window =5, min_count=1, sg=0)\n",
        "word_vectors = model.wv\n",
        "\n",
        "words = list(word_vectors.index_to_key)[:500]\n",
        "vectors = [word_vectors[word] for word in words]"
      ],
      "metadata": {
        "id": "7YzfZFKkdBfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters= 3) #3 küme\n",
        "kmeans.fit(vectors)\n",
        "clusters = kmeans.labels_ #her kelimenin hangi kümeye ait olduğunu gösterir. örn: [0,0,1,1,2..]\n",
        "\n",
        "pca = PCA(n_components = 2) #analizi kolaylaştırmak için 50 boyutu 2 boyuta indiriyoruz.\n",
        "reduced_vectors = pca.fit_transform(vectors)"
      ],
      "metadata": {
        "id": "R0HjEDvnfywb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2d görselleştirme\n",
        "plt.figure()\n",
        "plt.scatter(reduced_vectors[:,0],reduced_vectors[:,1],c = clusters,cmap =\"viridis\" )\n",
        "centers = pca.transform(kmeans.cluster_centers_)\n",
        "plt.scatter(centers[:,0], centers[:,1], c=\"red\", marker =\"x\", s=80, label = \"merkez\")\n",
        "plt.legend()\n",
        "\n",
        "for i, word in enumerate(words):\n",
        "  plt.text(reduced_vectors[i,0], reduced_vectors[i,1], word, fontsize=8)\n",
        "plt.title(\"word2Vec\")"
      ],
      "metadata": {
        "id": "rZJ1J7Imgjfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANSFORMERS TABANLI METİN TEMSİLİ\n",
        "\n",
        "\n",
        "*   BERT : (Bidirectional Encoder Representation from Transformers) bir kelimeyi incelerken önceki ve sonraki kelimelere de bakar. Metin sınıflandırma, adlandırılmış varlık tanıma, soru yanıtlama gibi görevlerde kullanılmaktadır.\n",
        "*   GPT : (Generative Pre-trained Transformer) Tek yönlü çalışır. Bir kelimeyi tahmin ederken önceki kelimelere dayanır (autoregressive).\n",
        "\n",
        "**Attention:** modelin belirli girdi parçalarına farklı derecelerde dikkat göstermesini sağlar.Özellikle bir kelimenin diğer kelimelerle olan ilişkisini anlamak için kullanılır.\n",
        "Sorgu (Query) ve Anahtar(Key) Çarpımı : Cümledeki kelimelerin birbiriyle olan ilişki skoru hesaplanır.\n",
        "\n",
        "**Input Embedding :** Girdi verilerini sayısal formata çevirmek için kullanılan tekniktir.\n",
        "\n",
        "**Multi Head Attention:** Birden fazla dikkat başlığı ile çalışır. Anlam ilişkisi, gramatik ilişki, cümledeki konumu gibi.\n",
        "\n",
        "**Masked multi head attention :** Model sadece geçmiş bilgileri kullanarak tahminde bulunur. Gelecekteki kelimeleri tahmin etmez.\n",
        "\n",
        "**Addition & Normalization  :** Dikkat katmanının çıktısına giriş verilerini ekler ve ardından verileri normalleştirir.\n",
        "\n",
        "**Feed Forward (İleri Besleme):** Her kelimeye ayrı ayrı uygulanan bir sinir ağı katmanıdır. Her encoder ve decoder katmanında bulunan bir ağdır.\n",
        "\n",
        "**Output Embedding :** Çıktı kelimelerini veya sembollerini sayısal vektörlere dönüştürür."
      ],
      "metadata": {
        "id": "nob5GfQoKAOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "text = \"Transformers are amazing for natural language processing.\"\n",
        "#tokenizasyon\n",
        "inputs  = tokenizer(text,return_tensors = \"pt\")\n",
        "with torch.no_grad():\n",
        "   #geri yayılım iptal edilmiş oldu. Ağırlıklar güncellenmeyecek.\n",
        "   outputs = model(**inputs)\n",
        "\n",
        "#çıkışlardan ilk tokenları alalım\n",
        "last_hidden_state = outputs.last_hidden_state\n",
        "first_token_embedding = last_hidden_state[0,0,:].numpy()"
      ],
      "metadata": {
        "id": "lrQqNd6cTZ04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"metin temsili ilk token : \")\n",
        "print(first_token_embedding)"
      ],
      "metadata": {
        "id": "mip3Q_fWY93H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = [\n",
        "    \"I love they\",\n",
        "    \"they love apple and banana\",\n",
        "    \"she loves you\",\n",
        "    \"They love me\"\n",
        "]\n",
        "\n",
        "tokens = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "tokens"
      ],
      "metadata": {
        "id": "mxDhmFS1tdcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = []\n",
        "for token_list in tokens:\n",
        "  bigrams.extend(list(ngrams(token_list,2)))\n",
        "\n",
        "bigrams"
      ],
      "metadata": {
        "id": "gW3i0qUruwoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams_freq = Counter(bigrams)\n",
        "bigrams_freq\n",
        "#kelimelerin bigramlarını oluşturur ve ard arda gelme sıklığını gösterir."
      ],
      "metadata": {
        "id": "ikBkoG0wERW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = []\n",
        "for token_list in tokens:\n",
        "  trigrams.extend(list(ngrams(token_list,3)))\n",
        "\n",
        "trigrams_freq = Counter(trigrams)\n",
        "trigrams_freq"
      ],
      "metadata": {
        "id": "uPLNe-yWElRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram = (\"they\",\"love\")\n",
        "prob_apple = trigrams_freq[(\"they\", \"love\", \"apple\")]/bigrams_freq[bigram]\n",
        "prob_me = trigrams_freq[(\"they\", \"love\", \"me\")]/bigrams_freq[bigram]"
      ],
      "metadata": {
        "id": "JerihPevF5An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hidden Markow model**\n"
      ],
      "metadata": {
        "id": "otCuEOkQeCK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "train_data = [\n",
        "    [(\"I\",\"PRP\"),(\"am\",\"VBP\"),(\"a\",\"DT\"),(\"student\",\"NN\")],\n",
        "    [(\"you\",\"PRP\"),(\"are\",\"VBP\"),(\"a\",\"DT\"),(\"teacher\",\"NN\")]\n",
        "]\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train(train_data)\n",
        "test_sentence = \"I am a teacher\".split()\n",
        "tags = hmm_tagger.tag(test_sentence)\n",
        "print(\"etiketli cümle :\",tags)"
      ],
      "metadata": {
        "id": "8rcZxeIBUV2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import conll2000\n",
        "nltk.download(\"conll2000\")\n",
        "train_data = conll2000.tagged_sents(\"train.txt\")\n",
        "test_data = conll2000.tagged_sents(\"test.txt\")\n",
        "\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train(train_data)\n",
        "test_sentence = \"I like going to park\".split()\n",
        "tags = hmm_tagger.tag(test_sentence)\n",
        "tags"
      ],
      "metadata": {
        "id": "bWu6peD-a9BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Maksimum Entropi Modelleri**\n"
      ],
      "metadata": {
        "id": "Jn73UTQ-ei07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify import MaxentClassifier\n",
        "train_data = [\n",
        "    ({\"love\":True, \"amazing\":True}, \"positive\"),\n",
        "    ({\"hate\":True, \"terrible\":True}, \"negative\"),\n",
        "    ({\"happy\":True, \"joy\":True}, \"positive\"),\n",
        "    ({\"sad\":True, \"depressed\":True}, \"negative\")\n",
        "]\n",
        "\n",
        "classifier = MaxentClassifier.train(train_data, max_iter = 10)\n",
        "test_sentence = \"I like this amazing movie\"\n",
        "\n",
        "features = {word: (word in test_sentence.lower().split()) for word in [\"love\",\"hate\",\"terrible\",\"happy\",\"joy\",\"sad\",\"depressed\"]}\n",
        "\n",
        "label = classifier.classify(features)\n",
        "label"
      ],
      "metadata": {
        "id": "DnHUtJwMe2Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WORD EMBEDDİNGS\n",
        "\n",
        "\n",
        "**WORD2VEC Temel Modelleri**\n",
        "*  CBOW\n",
        "\n",
        "bir kelimenin bağlamındaki diğer kelimeleri kullanarak o kelimeyi tahmin etmeyi hedefler.\n",
        "*   Skip-gram model\n",
        "\n",
        "CBOW'un tam tersidir. Bir kelimeyi kullanarak o kelimenin bağlamında yer alan kelimeleri tahmin etmeyi hedefler.\n",
        "\n"
      ],
      "metadata": {
        "id": "KzbSSof1MhVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recurrent Neural Networks (RNN)**\n",
        "\n",
        "*her zaman adımında önceki zaman adımındaki bilgiyi saklayarak ve sonraki adımlarla bu bilgiyi güncelleyerek çalışırlar.*\n",
        "\n"
      ],
      "metadata": {
        "id": "Cku6gw4LTH6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import SimpleRNN, Dense , Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I absolutely loved the movie, it was fantastic!\",\n",
        "        \"The plot was boring and predictable.\",\n",
        "        \"Great performances by the lead actors.\",\n",
        "        \"I didn't enjoy the film at all.\",\n",
        "        \"The cinematography was stunning and beautiful.\",\n",
        "        \"Terrible script and poor direction.\",\n",
        "        \"An emotional rollercoaster that kept me engaged.\",\n",
        "        \"The movie was too long and dragged on.\",\n",
        "        \"A masterpiece that exceeded my expectations.\",\n",
        "        \"I regret watching this movie.\",\n",
        "        \"The soundtrack was amazing and memorable.\",\n",
        "        \"The acting was wooden and unconvincing.\",\n",
        "        \"I laughed throughout, such a fun movie!\",\n",
        "        \"I almost fell asleep, it was that dull.\",\n",
        "        \"Brilliant storytelling and plot twists.\",\n",
        "        \"Disappointing and underwhelming.\",\n",
        "        \"The visuals were breathtaking and immersive.\",\n",
        "        \"Horrible pacing and confusing plot.\",\n",
        "        \"I was moved by the heartfelt scenes.\",\n",
        "        \"It felt like a waste of time.\",\n",
        "        \"Excellent character development.\",\n",
        "        \"Poor editing and awkward transitions.\",\n",
        "        \"The movie had a perfect balance of humor and drama.\",\n",
        "        \"I couldn't relate to any of the characters.\",\n",
        "        \"An inspiring and uplifting story.\",\n",
        "        \"The dialogues were cringe-worthy.\",\n",
        "        \"Highly entertaining and captivating from start to finish.\",\n",
        "        \"It was cliché and uninspired.\",\n",
        "        \"I would definitely watch it again.\",\n",
        "        \"I wish I hadn't spent money on this film.\"\n",
        "    ],\n",
        "    \"label\": [\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\",\n",
        "        \"positive\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "# tokenize\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df[\"text\"])\n",
        "sequences = tokenizer.texts_to_sequences(df[\"text\"])\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# padding\n",
        "maxlen = max(len(seq) for seq in sequences)\n",
        "x= pad_sequences(sequences,maxlen=maxlen)\n",
        "\n",
        "# label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y=label_encoder.fit_transform(data[\"label\"])\n",
        "\n",
        "#train_test_split\n",
        "X_train ,X_test, y_train, y_test = train_test_split(x ,y ,test_size= 0.3, random_state=42)\n",
        "\n",
        "# Word embedding\n",
        "sentences = [text.split() for text in data[\"text\"]]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix=np.zeros((len(word_index) +1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if word in word2vec_model.wv:\n",
        "    embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "#build RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(word_index)+1, output_dim = embedding_dim, weights= [embedding_matrix],input_length = maxlen, trainable = False))\n",
        "model.add(SimpleRNN(100,return_sequences=False))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(X_train,y_train,epochs= 10, batch_size = 2,validation_data=(X_test,y_test))\n",
        "\n",
        "print(\" \")\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test loss: \", loss)\n",
        "print(\"Test accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "ILFZa9o9CILS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(sentence):\n",
        "  seq = tokenizer.texts_to_sequences([sentence])\n",
        "  padded_seq = pad_sequences(seq,maxlen = maxlen)\n",
        "\n",
        "  prediction = model.predict(padded_seq)\n",
        "  predicted_class = (prediction > 0.5).astype(int)\n",
        "  label = \"pozitif\" if predicted_class[0][0] == 1 else \"negatif\"\n",
        "  return label\n",
        "\n",
        "sentence = \"The dialogues were cringe-worthy.\"\n",
        "result = classify_sentence(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "6AkTCt6AXvnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}